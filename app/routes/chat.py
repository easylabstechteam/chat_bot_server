# ------------------- IMPORTS -------------------
from fastapi import APIRouter
from repositories.redis_database.redis_repository import RedisRepository
from repositories.google_genai_llm.google_genai_llm import GoogleGeminaiRepository
from routes.validations.input.chat_input import ChatInput
from repositories.validations.redis_database.input.delete_chat_data_input import (
    DeleteChatDataInput,
)
from repositories.validations.redis_database.input.create_chat_data_input import (
    CreateChatDataInput,
)
from repositories.validations.redis_database.input.get_chat_data_input import (
    GetChatDataInput,
)
from repositories.validations.redis_database.input.update_chat_data_input import (
    UpdateChatDataInput,
)

# Create a new FastAPI router for chat-related endpoints
router = APIRouter()

# ------------------- HELPER NOTES -------------------
# ChatInput JSON example:
# {
#   "ip_adress_timestamp": "123e4567-e89b-12d3-a456-426614174001",
#   "session_id": "123e4567-e89b-12d3-a456-426614174000",  # Generated by backend when chat starts
#   "message": "Hello, how are you?"                        # User's message; validated for min/max length
# }

# ------------------- ROUTES -------------------


@router.post("/user/chat", summary="Chat with the LLM and detect intent")
async def chat(user_message: ChatInput):

    # Handles a single user message in an ongoing chat session.

    # Steps:
    # 1. Save the user's message to Redis.
    # 2. Detect the user's intent using Google GeminAI.
    # 3. Retrieve the chat history for context.
    # 4. Generate the LLM response based on chat history and detected intent.
    # 5. Save the LLM response to Redis.
    # 6. Return the LLM response to the frontend.

    # Step 1: Save the incoming user message in Redis
    await RedisRepository.update_chat_data(user_message)

    # Step 2: Detect the user's intent using the AI service
    intent_detected = await GoogleGeminaiRepository.intent_detector(
        user_message=user_message
    )

    # Step 3: Retrieve previous chat messages for context
    chat_history = await RedisRepository.get_chat_data(user_message.session_id)

    # Step 4: Continue conversation with the LLM using context and detected intent
    llm_response = await GoogleGeminaiRepository.chatbot_continue_conversation(
        session_id=user_message.session_id,
        chat_history=chat_history,
        questions=intent_detected.questions,
    )

    # Step 5: Save the LLM's response to Redis
    await RedisRepository.update_chat_data(llm_response)

    # Step 6: Return the AI response to the frontend
    return llm_response


@router.post("/chats/start", summary="Start a new chat session")
async def initiate_chat(user_message: ChatInput):

    # Initiates a new chat session.

    # Backend responsibilities:
    # - Generate a new session_id.
    # - Save the first user message in Redis.
    # - Return the session_id and confirmation to the frontend.

    return await RedisRepository.create_chat_data(user_message=user_message)


@router.post("/chats/get/messages", summary="Retrieve chat history")
async def get_chat_messages(input: GetChatDataInput):

    # Retrieves all messages for a given chat session.

    # Input:
    # - session_id: UUID of the chat session

    # Returns:
    # - List of messages for the session

    return await RedisRepository.get_chat_data(session_id=input.session_id)


@router.delete("/chats/delete", summary="Delete a chat session")
async def delete_chat(input: DeleteChatDataInput):

    # Deletes an entire chat session from Redis.

    # Input:
    # - session_id: UUID of the session to delete

    # Returns:
    # - Success/failure status

    return await RedisRepository.delete_chat_data(session_id=input.session_id)


@router.get("/", summary="Home endpoint")
async def home():

    # Simple home endpoint to verify that the service is running.

    return {"msg": "home"}
